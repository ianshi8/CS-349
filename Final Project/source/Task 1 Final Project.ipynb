{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS 349 Final Project\n",
    "Ian Shi, Ellen Liao, Tim Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, RFECV, chi2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads for python nltk (sentiment analysis)\n",
    "\n",
    "\"\"\"\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('all')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading in preprocessed dataset to save time for analysis\n",
    "# prod_train = pd.read_json(\"./preprocessed_data.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CD_rev_train = pd.read_json(\"./CDs_and_Vinyl/train/review_training.json\")\n",
    "CD_prod_train = pd.read_json(\"./CDs_and_Vinyl/train/product_training.json\")\n",
    "# set prod_train to preprocessed data so we don't have to rerun processing every time\n",
    "prod_train = pd.read_json(\"./preprocessed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(list(CD_rev_train.columns))\n",
    "print(list(CD_prod_train.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data set containing specific asin (product) and number of unique reviews \n",
    "prod_reviews = CD_rev_train.groupby('asin')['reviewerID'].nunique()\n",
    "\n",
    "# data set containing specific asin (product) and average review upvote\n",
    "CD_rev_train['vote'] = pd.to_numeric(CD_rev_train['vote'], errors='coerce')\n",
    "CD_rev_train['vote'] = CD_rev_train['vote'].fillna(0).astype(int)\n",
    "prod_votes = CD_rev_train.groupby('asin')['vote'].mean()\n",
    "\n",
    "# data set containing specific asin (product) and proportion of verified reviews\n",
    "prod_ver = CD_rev_train.groupby('asin')['verified'].mean()\n",
    "\n",
    "# data set containing specific asin (product) and proportion of reviews that include images\n",
    "CD_rev_train['image'] = CD_rev_train['image'].notna()\n",
    "prod_image = CD_rev_train.groupby('asin')['image'].mean()\n",
    "\n",
    "# data set containing specific asin (product) and time of earliest review\n",
    "earliest_rev = CD_rev_train.groupby('asin')['unixReviewTime'].min()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis using Python nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentiment analysis preprocessing\n",
    "def preprocess(text):\n",
    "    # creating tokens and formatting\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    # removing stop words from nltk package of stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # lemmatizing (grouping tg different inflected forms of same word)\n",
    "    lemmatized = [WordNetLemmatizer().lemmatize(token) for token in stop_tokens]\n",
    "    # join tokens tg\n",
    "    processed = ' '.join(lemmatized)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reviewText are empty, making sure those get converted to empty string\n",
    "CD_rev_train['reviewText'] = CD_rev_train['reviewText'].fillna('')\n",
    "CD_rev_train['reviewText'] = CD_rev_train['reviewText'].apply(preprocess)\n",
    "CD_rev_train['reviewText']\n",
    "CD_rev_train['summary'] = CD_rev_train['summary'].fillna('')\n",
    "CD_rev_train['summary'] = CD_rev_train['summary'].apply(preprocess)\n",
    "CD_rev_train['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# running nltk sentiment analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def comp_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = scores['compound']\n",
    "    return sentiment\n",
    "\n",
    "def sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    if scores['compound'] > 0:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment\n",
    "\n",
    "def pos_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = scores['pos']\n",
    "    return sentiment\n",
    "\n",
    "def neg_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = scores['neg']\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running nltk sentiment analysis, criteria in the sentiment function could definitely change\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# just raw polarity score output (and then averaging them across all reviews for certain prod later)\n",
    "# analyzer.polarity_scores returns -1 to 1\n",
    "def sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    # looking through nltk documentation, 'compound' is the aggregate sentiment\n",
    "    # 'pos' gives positive sentiment only (so negative sentiment isn't subtracted)\n",
    "    sentiment = scores['compound']\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# applying sentiment analysis to dataset\n",
    "# compound sentiment values\n",
    "CD_rev_train['rev_compSentiment'] = CD_rev_train['reviewText'].apply(comp_sentiment)\n",
    "CD_rev_train['summ_compSentiment'] = CD_rev_train['summary'].apply(comp_sentiment)\n",
    "\n",
    "# 1 or 0 sentiment values\n",
    "CD_rev_train['rev_Sentiment'] = CD_rev_train['reviewText'].apply(sentiment)\n",
    "CD_rev_train['summ_Sentiment'] = CD_rev_train['summary'].apply(sentiment)\n",
    "\n",
    "# pos sentiment values\n",
    "CD_rev_train['rev_posSentiment'] = CD_rev_train['reviewText'].apply(pos_sentiment)\n",
    "CD_rev_train['summ_posSentiment'] = CD_rev_train['summary'].apply(pos_sentiment)\n",
    "\n",
    "# neg sentiment values\n",
    "CD_rev_train['rev_negSentiment'] = CD_rev_train['reviewText'].apply(neg_sentiment)\n",
    "CD_rev_train['summ_negSentiment'] = CD_rev_train['summary'].apply(neg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset containing proportion of postively sentimented reviews and summaries for each product\n",
    "rev_compSentiment = CD_rev_train.groupby('asin')['rev_compSentiment'].mean()\n",
    "summ_compSentiment = CD_rev_train.groupby('asin')['summ_compSentiment'].mean()\n",
    "\n",
    "rev_posSentiment = CD_rev_train.groupby('asin')['rev_posSentiment'].mean()\n",
    "summ_posSentiment = CD_rev_train.groupby('asin')['summ_posSentiment'].mean()\n",
    "\n",
    "rev_negSentiment = CD_rev_train.groupby('asin')['rev_negSentiment'].mean()\n",
    "summ_negSentiment = CD_rev_train.groupby('asin')['summ_negSentiment'].mean()\n",
    "\n",
    "rev_Sentiment = CD_rev_train.groupby('asin')['rev_Sentiment'].mean().apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "summ_Sentiment = CD_rev_train.groupby('asin')['summ_Sentiment'].mean().apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combining datasets into one\n",
    "prod_train = pd.concat([prod_reviews, prod_votes, prod_ver, rev_Sentiment, summ_Sentiment, rev_compSentiment, summ_compSentiment,\n",
    "                        rev_posSentiment, summ_posSentiment, rev_negSentiment, summ_negSentiment, prod_image], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge expected awesomeness into dataset\n",
    "prod_train = prod_train.merge(CD_prod_train, left_index = True, right_on = 'asin')\n",
    "prod_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading preprocessed training data to separate json - i have the git repo cloned to a folder\n",
    "# prod_train.to_json(r'/Users/IanShi/Desktop/Classwork/CS 349/cs349-project/preprocessed_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive to negative sentiment ratio\n",
    "prod_train['rev_posNegRatio'] = (prod_train['rev_posSentiment'] + 1) \\\n",
    "    / (prod_train['rev_negSentiment'] + 1)\n",
    "prod_train['summ_posNegRatio'] = (prod_train['summ_posSentiment'] + 1) \\\n",
    "    / (prod_train['summ_negSentiment'] + 1)\n",
    "\n",
    "# summary to review sentiment ratio\n",
    "prod_train['summToRev'] = (prod_train['summ_compSentiment'] + 1) \\\n",
    "    / (prod_train['rev_compSentiment'] + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Models\n",
    "### Gaussian Naive Bayes\n",
    "Below is a coded naive bayes function that we tested at first. Later switched to sklearn's gnb model for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian naive-bayes setup\n",
    "mean_by_awe = prod_train.groupby('awesomeness').mean(numeric_only=True)\n",
    "var_by_awe = prod_train.groupby('awesomeness').var(numeric_only=True)\n",
    "count_by_awe = prod_train.groupby('awesomeness')['asin'].count()\n",
    "\n",
    "# gaussian naive-bayes\n",
    "features = list(mean_by_awe.columns)\n",
    "features.pop(12)\n",
    "def gaussian_nb(feature_vec: pd.Series):\n",
    "    prob = [count_by_awe.at[0], count_by_awe.at[1]]\n",
    "    for feature in features:\n",
    "        prob[0] *= (1 / math.sqrt(2 * math.pi * var_by_awe.at[0,feature])) \\\n",
    "        * math.exp(-pow((feature_vec.at[feature] - mean_by_awe.at[0,feature]), 2) \\\n",
    "                   / (2 * pow(var_by_awe.at[0,feature], 2)))\n",
    "        prob[1] *= (1 / math.sqrt(2 * math.pi * var_by_awe.at[1,feature])) \\\n",
    "        * math.exp(-pow((feature_vec.at[feature] - mean_by_awe.at[1,feature]), 2) \\\n",
    "                   / (2 * pow(var_by_awe.at[1,feature], 2)))\n",
    "    if prob[0] > prob[1]:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply gaussian naive-bayes to dataset\n",
    "gaussianNB = prod_train.apply(gaussian_nb, axis=1)\n",
    "\n",
    "# compute accuracy, f1\n",
    "num_correct = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i in range(len(prod_train)):\n",
    "    if (gaussianNB[i] == prod_train['awesomeness'][i]):\n",
    "        num_correct = num_correct + 1\n",
    "        if gaussianNB[i] == 1:\n",
    "            tp += 1\n",
    "    elif gaussianNB[i] == 1:\n",
    "        fp += 1\n",
    "    elif gaussianNB[i] == 0:\n",
    "        fn += 1\n",
    "print(\"Gaussian NB Accuracy\", num_correct/len(prod_train))\n",
    "print(\"Gaussian NB F1 Score\", 2*tp/(2*tp + fp + fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian nb setup\n",
    "features = list(mean_by_awe.columns)\n",
    "features.pop(12)\n",
    "X = prod_train[features]\n",
    "Y = prod_train['awesomeness']\n",
    "\n",
    "# splitting data into train and test (70/30)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "# gaussian nb with sklearn\n",
    "# use recursive feature elimination with cv (RFECV) to find best features\n",
    "# results: ['verified', 'unixReviewTime', 'rev_negSentiment', 'summ_negSentiment']\n",
    "gnb = GaussianNB()\n",
    "#rfecv = RFECV(estimator=gnb, scoring='f1', n_jobs=-1, cv=10, verbose=2)\n",
    "gnb.fit(X_train, Y_train)\n",
    "gnb_pred = gnb.predict(X_test)\n",
    "#print(\"Features:\", rfecv.get_feature_names_out(features))\n",
    "#gnb_cv = pd.DataFrame(rfecv.cv_results_)\n",
    "\n",
    "# model accuracy\n",
    "print(\"Gaussian NB Accuracy:\",metrics.accuracy_score(Y_test, gnb_pred))\n",
    "\n",
    "# 10 fold cross validation\n",
    "gnb_cv = cross_val_score(gnb, X, Y, cv = 10, scoring='f1')\n",
    "print(\"Mean CV F1 Score:\", np.mean(gnb_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(gnb_cv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree setup\n",
    "# features = ['reviewerID', 'vote', 'verified', 'rev_Sentiment', 'summ_Sentiment', 'rev_compSentiment', 'summ_compSentiment',\n",
    "#             'rev_posSentiment', 'summ_posSentiment', 'rev_negSentiment', 'summ_negSentiment', 'image']\n",
    "#features = ['reviewerID', 'vote', 'verified', 'rev_Sentiment', 'summ_Sentiment', 'image']\n",
    "features = list(mean_by_awe.columns)\n",
    "features.pop(12)\n",
    "X = prod_train[features]\n",
    "Y = prod_train['awesomeness']\n",
    "\n",
    "# splitting data into train and test (70/30)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "'''\n",
    "# use grid search to find best hyperparameters\n",
    "# the results are criterion: gini/entropy (~same), max_depth: 3\n",
    "param_grid = {\"criterion\": ['gini', 'entropy'], \"max_depth\": [x for x in range(1,11)]}\n",
    "tree = DecisionTreeClassifier()\n",
    "gs = GridSearchCV(estimator=tree, param_grid=param_grid, scoring='f1', n_jobs=-1, cv=10, verbose=2)\n",
    "gs.fit(X_train, Y_train)\n",
    "tree_pred = gs.predict(X_test)\n",
    "tree_cv = pd.DataFrame(gs.cv_results_)\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "# use recursive feature elimination with cv (RFECV) to find best features\n",
    "# results: ['verified', 'unixReviewTime', 'rev_negSentiment', 'summ_negSentiment']\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth = 3)\n",
    "rfecv = RFECV(estimator=tree, scoring='f1', n_jobs=-1, cv=10, verbose=0)\n",
    "rfecv.fit(X_train, Y_train)\n",
    "tree_pred = rfecv.predict(X_test)\n",
    "print(\"Features:\", rfecv.get_feature_names_out(features))\n",
    "tree_cv = pd.DataFrame(rfecv.cv_results_)\n",
    "\"\"\"\n",
    "\n",
    "X_train = X_train[['verified', 'unixReviewTime', 'rev_negSentiment', 'summ_negSentiment']]\n",
    "X_test = X_test[['verified', 'unixReviewTime', 'rev_negSentiment', 'summ_negSentiment']]\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth = 3)\n",
    "tree = tree.fit(X_train, Y_train)\n",
    "tree_pred = tree.predict(X_test)\n",
    "\n",
    "# model accuracy\n",
    "print(\"Decision Tree Accuracy:\",metrics.accuracy_score(Y_test, tree_pred))\n",
    "\n",
    "# 10 fold cross validation\n",
    "tree_cv = cross_val_score(tree, X, Y, cv = 10, scoring='f1')\n",
    "print(\"Mean CV F1 Score:\", np.mean(tree_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(tree_cv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "# trying RandomizedSearchCV to find best parameters\n",
    "'''\n",
    "hyperparams = {'n_estimators': randint(20,200),\n",
    "              'max_depth': randint(1,20)}\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "rand_search = RandomizedSearchCV(rf, param_distributions = hyperparams, n_iter=10, scoring='f1', cv=10, n_jobs=-1)\n",
    "rand_search.fit(X_train, Y_train)\n",
    "best_rf = rand_search.best_estimator_\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "best_pred = best_rf.predict(X_test)\n",
    "#10 fold cross validation\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(Y_test, best_pred))\n",
    "forest_cv = cross_val_score(best_rf, X, Y, cv = 10, scoring='f1', n_jobs=-1)\n",
    "print(\"Mean CV F1 Score:\", np.mean(forest_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(forest_cv))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RFECV for feature selection\n",
    "'''\n",
    "rf_estimator = RandomForestClassifier(n_estimators = 15, max_depth = 1, n_jobs=-1)\n",
    "rf_selector = RFECV(estimator = rf_estimator, scoring = 'f1', step = 1, cv = 5, verbose = 2)\n",
    "rf_selector = rf_selector.fit(X_train, Y_train)\n",
    "rf_selector.ranking_\n",
    "# selected features ['verified','rev_posSentiment', 'rev_negSentiment', 'summ_negSentiment', 'rev_posNegRatio', 'summ_posNegRatio', 'summToRev']\n",
    "'''\n",
    "\n",
    "X_train = X_train[['verified','rev_posSentiment', 'rev_negSentiment', 'summ_negSentiment', 'rev_posNegRatio', 'summ_posNegRatio', 'summToRev']]\n",
    "X_test = X_test[['verified','rev_posSentiment', 'rev_negSentiment', 'summ_negSentiment', 'rev_posNegRatio', 'summ_posNegRatio', 'summToRev']]\n",
    "rf = RandomForestClassifier(n_estimators = 28, max_depth = 1, n_jobs=-1)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(Y_test, rf_pred))\n",
    "forest_cv = cross_val_score(rf, X, Y, cv = 10, scoring='f1', n_jobs=-1)\n",
    "print(\"Mean CV F1 Score:\", np.mean(forest_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(forest_cv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm \n",
    "svm_class = svm.SVC()\n",
    "svm_class = svm_class.fit(X_train, Y_train)\n",
    "svm_pred = svm_class.predict(X_test)\n",
    "\n",
    "# model accuracy\n",
    "print(\"SVM Tree Accuracy:\",metrics.accuracy_score(Y_test, svm_pred))\n",
    "\n",
    "# 10 fold cross validation\n",
    "svm_cv = cross_val_score(svm_class, X, Y, cv = 10, scoring='f1', n_jobs=-1, verbose=2)\n",
    "print(\"Mean CV F1 Score:\", np.mean(svm_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(svm_cv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-nearest neighbors\n",
    "kneigh = KNeighborsClassifier(n_neighbors = 9, p=1, n_jobs=-1)\n",
    "\n",
    "# set up\n",
    "#features = ['reviewerID', 'vote', 'verified', 'rev_Sentiment', 'summ_Sentiment', 'rev_posSentiment', 'summ_posSentiment', 'rev_negSentiment', 'summ_negSentiment', 'image']\n",
    "features = list(mean_by_awe.columns)\n",
    "features.pop(12) # remove \"image\" feature for now -- currently broken -ellen\n",
    "X = prod_train[features]\n",
    "Y = prod_train['awesomeness']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "\"\"\"\n",
    "# use grid search to find best hyperparameters\n",
    "# results:  n_neighbors: 9, weights: uniform, p: 1\n",
    "param_grid = {\"n_neighbors\": [x for x in range(1,11)],\n",
    "              \"weights\": ['uniform', 'distance'],\n",
    "              \"p\": [1, 2]}\n",
    "kneigh = KNeighborsClassifier(n_jobs=-1)\n",
    "gs = GridSearchCV(estimator=kneigh, param_grid=param_grid, scoring='f1', n_jobs=-1, cv=10, verbose=2)\n",
    "gs.fit(X_train, Y_train)\n",
    "kneigh_pred = gs.predict(X_test)\n",
    "kneigh_cv = pd.DataFrame(gs.cv_results_)\n",
    "\"\"\"\n",
    "\n",
    "# train the model\n",
    "kneigh.fit(X_train, Y_train)\n",
    "\n",
    "# apply model to test set\n",
    "kneigh_pred = kneigh.predict(X_test)\n",
    "kneigh_pred_prob = kneigh.predict_proba(X_test)\n",
    "\n",
    "# mean accuracy\n",
    "kneigh_score = kneigh.score(X_test, Y_test)\n",
    "\n",
    "# 10 fold cross validation\n",
    "kneigh_cv = cross_val_score(kneigh, X, Y, cv = 10, scoring='f1', n_jobs=-1)\n",
    "print(\"Mean CV F1 Score:\", np.mean(kneigh_cv))\n",
    "print(\"Standard Deviation F1 Score:\", np.std(kneigh_cv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
